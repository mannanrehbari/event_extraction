{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64333c1d",
   "metadata": {},
   "source": [
    "**Event Extraction with Reinforcement Learning**\n",
    "\n",
    "- This part of event extraction process uses Reinforcement Learning. \n",
    "- The process comprises of a main-task and a subsidiary-task.\n",
    "\n",
    "\n",
    "*Main-Task: Trigger Identification*\n",
    "- The main-task is also referred as trigger identification. In this task, the Agent sequentially scans tokens in the input sentence. \n",
    "- At each time step, the event type is assigned to the corresponding token according to a stochastic policy in RL.\n",
    "- If the current word is recognized as an event trigger, a subsidiary-task will be launched.\n",
    "- If the current word has an event type of 'None', the agent will skip the argument detection sub-task.\n",
    "\n",
    "*Subsidiary-Task: Argument Detection*\n",
    "- When a token is marked as event trigger, a subsidiary task is launched.\n",
    "- This task detects the arguments in the event that corresponds to the event trigger. \n",
    "- The performance of this subtask is used to compute the reward for the action taken in the main-task.\n",
    "\n",
    "**Extraction Process Steps**\n",
    "\n",
    "The entire process is depicted in the following diagram and explained in steps: \n",
    "\n",
    "> ![BMEE](BMEE.png \"Biomedical Event Extraction\")\n",
    "> * Current token in sentence is identified as event trigger \n",
    "> * The vector of trigger word and the embedded environment information will be concatenated and fed to Agent\n",
    "> * The Agent takens an action, representing the predicted event type for word, according to a stochastic policy.\n",
    "> * The word embedding is then concatenated with action vector to create a new representation, for all words in the input sentence.\n",
    "> * This new representation of the input sentence pass through a BiLSTM-CRF module which detects the arguments given the indentified event trigger.\n",
    "> * Then the predicted result of argument detection Y' is compared with ground-truth Y to compute a Reward.\n",
    "> * Additionally, Y' is transformed into a vector L by a BiLSTM layer which is concatenated to environment information. This ends up helping the subsequent trigger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065bbf3",
   "metadata": {},
   "source": [
    "**Trigger Identification**\n",
    "\n",
    "- Trigger identification is the main task in the Reinforcement Learning part of the implementation\n",
    "- This Module will read the sentences in the text, break a sentence into tokens, use the embedding generated by the KB Augmented Representation module to detect if a given token in a sentence is a trigger or not. \n",
    "- The set of possible actions include all identified event types and an additional 'None' type if a token is non-trigger.\n",
    "- We will be utilizing a few classes that will read and parse the input data, and prepare the text data and provided annotations for RL environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c750f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install scikit-learn\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5990f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 21:50:45.025150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import sqlite3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af346836",
   "metadata": {},
   "source": [
    "**Embeddings for tokens**\n",
    "\n",
    "- The following few steps, we have a bunch of classes together that will provide an embedding for each token in the sentence.\n",
    "- These include GloVe word embedding [100], Parts-of-Speech tag embedding, Entity Type embedding, and KBs concept embedding, in order. \n",
    "- All of these embeddings are concatenated PER TOKEN in the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c47f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeWordEmbedder:\n",
    "    def __init__(self, pathGloveFile):\n",
    "        self.pathGloveFile = pathGloveFile\n",
    "        self.wordToVectorMap = {}\n",
    "        self.loadAndParseGloVeData()\n",
    "    \n",
    "    def loadAndParseGloVeData(self):\n",
    "        print(\"Creating wordToVectorMap for GloVe data:\")\n",
    "        startTime = time.time()\n",
    "        with open(self.pathGloveFile, 'r', encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                self.wordToVectorMap[word] = vector\n",
    "        print(\"Done - Creating wordToVectorMap for GloVe data\")\n",
    "        endTime = time.time()\n",
    "        loadTime = endTime - startTime\n",
    "        print(f\"Took {loadTime} seconds to read GloVe data\")\n",
    "    \n",
    "    def getWordEmbedding(self, word):\n",
    "        return self.wordToVectorMap.get(word.lower(), np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3367fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagEmbedder:\n",
    "    \n",
    "    def __init__(self, pathPOSmodel, pathJPOSar, dimension=50):\n",
    "        print(\"Initializing Parts-of-Speech Tagger\")\n",
    "        self.tagger = StanfordPOSTagger(pathPOSmodel, pathJPOSar, encoding='utf-8')\n",
    "        self.dimension = dimension\n",
    "        self.posEmbeddings = {}\n",
    "        print(\"Initialized Parts-of-Speech Tagger\")\n",
    "    \n",
    "    def tagSentence(self, sentence):\n",
    "        tokens = word_tokenize(sentence)\n",
    "        return self.tagger.tag(tokens)\n",
    "    \n",
    "    def getEmbedding(self, pos_tag):\n",
    "        if pos_tag not in self.posEmbeddings:\n",
    "            self.posEmbeddings[pos_tag] = np.random.rand(self.dimension)\n",
    "        return self.posEmbeddings[pos_tag]\n",
    "    \n",
    "    def embedSentence(self, sentence):\n",
    "        posTags = self.tagSentence(sentence)\n",
    "        posTagEmbeddings = [self.getEmbedding(tag) for _,tag in posTags]\n",
    "        return posTags, posTagEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d8ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityTypeEmbedderSTANDOFF:\n",
    "    \n",
    "    def __init__(self, inputDir, embeddingDim=50):\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.entityEmbeddings = {}\n",
    "        self.tokenEntityMappings = {}\n",
    "        self.entityTypes = set()\n",
    "        self.inputDir = inputDir\n",
    "        self.extractTokenEntityMapping()\n",
    "    \n",
    "    def extractTokenEntityMapping(self):\n",
    "        numFiles = 0\n",
    "        for dirpath, _, filenames in os.walk(self.inputDir):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".ann\"):\n",
    "                    numFiles += 1\n",
    "                    filePath = os.path.join(dirpath, filename)\n",
    "                    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "                        for line in file:\n",
    "                            if line.strip():  # non-empty line\n",
    "                                parts = line.split(\"\\t\")\n",
    "                                if len(parts) == 3 and parts[0][0] == \"T\":\n",
    "                                    token, entityTypeParts = parts[-1].strip(), parts[1]\n",
    "                                    middleParts = entityTypeParts.split()\n",
    "                                    entityType = middleParts[0]\n",
    "                                    self.tokenEntityMappings[token] = entityType\n",
    "                                    if entityType not in self.entityEmbeddings:\n",
    "                                        self.entityTypes.add(entityType)\n",
    "                                        self.entityEmbeddings[entityType] = np.random.rand(self.embeddingDim)\n",
    "        print(f\"Processed {numFiles} .ann files.\")\n",
    "        uniqueTokens = len(self.tokenEntityMappings)\n",
    "        uniqueEntities = len(self.entityTypes)\n",
    "        print(f\"Found {uniqueTokens} unique tokens which correspond to entities\")\n",
    "        print(f\"Found {uniqueEntities} unique entities\")\n",
    "\n",
    "    def get_embedding(self, token):\n",
    "        entityType = self.get_entity_type(token)\n",
    "        return self.entityEmbeddings.get(entityType, np.zeros(self.embeddingDim))\n",
    "    \n",
    "    def get_entity_type(self, token):\n",
    "        return self.tokenEntityMappings.get(token, 'None')\n",
    "    \n",
    "    def update_embedding(self, entityType, newEmbedding):\n",
    "        if entityType in self.entityEmbeddings:\n",
    "            self.entityEmbeddings[entityType] = newEmbedding\n",
    "        else:\n",
    "            raise ValueError(f\"Entity type {entityType} not in embeddings dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2005c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDefinitionEmbedder:\n",
    "    \n",
    "    def __init__(self, sqlitePath, table,pcaComponents=100, sentenceTransformerType=\"all-MiniLM-L6-v2\"):\n",
    "        self.entityGOdefinitionEmbedding = {}\n",
    "        self.sqliteConn = sqlite3.connect(sqlitePath)\n",
    "        self.table = table\n",
    "        self.pcaComponents = pcaComponents\n",
    "        self.sentenceTransformerType = sentenceTransformerType\n",
    "        \n",
    "    def readSQLiteData(self):\n",
    "        \n",
    "        print(\"Reading GO definitions from SQLite\")\n",
    "        cursor = self.sqliteConn.cursor()\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT entity, GO_definition\n",
    "        FROM {table}\n",
    "        WHERE GO_definition != '' AND GO_definition IS NOT NULL;\n",
    "        \"\"\".format(table=self.table)\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        self.rows = cursor.fetchall()\n",
    "        self.sqliteConn.close()\n",
    "        \n",
    "        print(f\"Done Reading {len(self.rows)} GO definitions from SQLite\")\n",
    "        \n",
    "        \n",
    "    def createEmbeddings(self):\n",
    "        print(\"Creating embeddings for GO_definitions\")\n",
    "        model = SentenceTransformer(self.sentenceTransformerType)\n",
    "        pca = PCA(n_components=self.pcaComponents)\n",
    "        \n",
    "        entities = []\n",
    "        GOdefs = []\n",
    "        \n",
    "        for row in self.rows:\n",
    "            entities.append(row[0])\n",
    "            GOdefs.append(row[1])\n",
    "        \n",
    "        definitionEmbeddings = model.encode(GOdefs)\n",
    "        reducedEmbeddings = pca.fit_transform(definitionEmbeddings)\n",
    "        \n",
    "        self.entityGOdefinitionEmbedding = dict(zip(entities, reducedEmbeddings))\n",
    "        print(f\"Done Creating embeddings for {len(entities)} GO_definitions corresponding to entities\")\n",
    "        \n",
    "    def getEmbeddingForEntity(self, entity):\n",
    "        return self.entityGOdefinitionEmbedding.get(entity, np.zeros(self.pcaComponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434fca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBAugmentedWordRepresentation:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.pathGloVe100dFile = '../BME Corpora/glove.6B/glove.6B.100d.txt'\n",
    "        \n",
    "        self.pathToPOSJar = \"../stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
    "        self.pathToPOSModel = \"../stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\"\n",
    "        \n",
    "        self.entityTypeInputDir = '../BME Corpora/MLEE-1.0.2-rev1/standoff/full/'\n",
    "        \n",
    "        self.sqlitePath = \"../QuickGO.db\"\n",
    "        self.standOffTable = \"QuickGOSTANDOFF\"\n",
    "        \n",
    "        self.initGloveEmbedder()\n",
    "        self.initEntityTypeEmbedder()\n",
    "        self.initPosTagEmbedder()\n",
    "    \n",
    "    def initGloveEmbedder(self):\n",
    "        self.gloVeEmbedder = GloVeWordEmbedder(self.pathGloVe100dFile)\n",
    "        \n",
    "    def initPosTagEmbedder(self):\n",
    "        self.posTagEmbedder = POSTagEmbedder(self.pathToPOSModel, self.pathToPOSJar)\n",
    "    \n",
    "    # this embedding module works in isolation - However, it ends up killing the kernel while integrating in the pipeline\n",
    "    def initKBConceptEmbedder(self):\n",
    "        self.kbConceptEmbedder = OntologyDefinitionEmbedder(self.sqlitePath, self.standOffTable)\n",
    "        self.kbConceptEmbedder.readSQLiteData()\n",
    "        self.kbConceptEmbedder.createEmbeddings()\n",
    "    \n",
    "    def initEntityTypeEmbedder(self):\n",
    "        self.entityTypeEmbedder = EntityTypeEmbedderSTANDOFF(self.entityTypeInputDir)\n",
    "    \n",
    "    def getKBRepresentationForToken(self, token):\n",
    "        reprGlove = self.gloVeEmbedder.getWordEmbedding(token)\n",
    "        reprEntityType = self.entityTypeEmbedder.get_embedding(token)\n",
    "        return np.concatenate((reprGlove, reprEntityType))\n",
    "    \n",
    "    def getPosTagForSentence(self, text):\n",
    "        return self.posTagEmbedder.embedSentence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e7aef",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2559bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationReader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def readAnnotations(self, filePath):\n",
    "        with open(filePath, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def parseAnnotations(self, content):\n",
    "        entities = {}\n",
    "        events = {}\n",
    "        \n",
    "        for line in content.strip().split('\\n'):\n",
    "            if line.startswith('T'):\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 3:\n",
    "                    continue;\n",
    "                entityId, entityInfo, _ = parts\n",
    "                entityType, start, end = entityInfo.split(' ')\n",
    "                entities[entityId] = {'type': entityType, 'span': (int(start), int(end))}\n",
    "            elif line.startswith('E'):\n",
    "                parts = line.split('\\t')\n",
    "                eventId, eventInfo = parts\n",
    "                eventInfoParts = eventInfo.split(' ')\n",
    "                if len(eventInfoParts) != 3:\n",
    "                    continue\n",
    "                eventTypeTrigger = eventInfoParts[0].split(':')\n",
    "                eventArgs = [arg.split(':') for arg in eventInfoParts[1:]]\n",
    "                events[eventId] = {'type': eventTypeTrigger[0], 'trigger': eventTypeTrigger[1], 'args': eventArgs}\n",
    "        return entities, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bbf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateRepresentation:\n",
    "    def __init__(self, kbEmbedder, tokens):\n",
    "        self.kbEmbedder = kbEmbedder\n",
    "        self.tokens = tokens\n",
    "    \n",
    "    def getEmbedding(self, token):\n",
    "        return self.kbEmbedder.getKBRepresentationForToken(token)\n",
    "    \n",
    "    def getState(self, currentIndex):\n",
    "        currentToken = self.tokens[currentIndex]\n",
    "        state = self.getEmbedding(currentToken)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120e4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    def __init__(self, stateSize=150, actionSize=2, hiddenSize=32, learningRate=0.001):\n",
    "        self.stateSize = stateSize\n",
    "        self.actionSize = actionSize\n",
    "        \n",
    "        # Build MLP for trigger identification\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hiddenSize, input_dim=stateSize, activation='relu'))\n",
    "        self.model.add(Dense(actionSize, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=Adam(learningRate))\n",
    "        \n",
    "    def chooseAction(self, state):\n",
    "        # Revise: action should be based on policy\n",
    "        # Trigger: 1, Not Trigger 0\n",
    "        state = np.reshape(state, [1, self.stateSize])\n",
    "        actionProbability = self.model.predict(state, verbose=0)[0]\n",
    "        action = np.random.choice(range(self.actionSize), p=actionProbability)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, nextState, done):\n",
    "        state = np.reshape(state, [1, self.stateSize])\n",
    "        if nextState is not None:\n",
    "            nextState = np.reshape(nextState, [1, self.stateSize])\n",
    "        else:\n",
    "            nextState = np.zeros([1, self.stateSize])\n",
    "            \n",
    "        # Predicted Q-values for the current state\n",
    "        currentQ = self.model.predict(state, verbose=0)[0]\n",
    "        \n",
    "        # Update Q-value for the action taken\n",
    "        currentQ[action] = reward\n",
    "        \n",
    "        self.model.fit(state, np.array([currentQ]), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efd5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLEnvironment:\n",
    "    def __init__(self, kbEmbedder, annotationReader):\n",
    "        self.directoryPath = \"../BME Corpora/MLEE-1.0.2-rev1/standoff/full/\"\n",
    "        self.kbEmbedder = kbEmbedder\n",
    "        self.annotationReader = annotationReader\n",
    "        \n",
    "        self.fileNames = self.fileList(\".ann\") # or could use .txt since they are in pairs\n",
    "        print(f\"Number of files to read:{len(self.fileNames)}\")\n",
    "        \n",
    "        self.fileIndex = 0\n",
    "        self.tokenIndex = 0\n",
    "        self.text = \"\"\n",
    "        \n",
    "        self.entities = None\n",
    "        self.events = None\n",
    "        \n",
    "        self.stateRepr = None\n",
    "        \n",
    "        self.totalIdAttempts = 0\n",
    "        self.correctIds = 0\n",
    "        \n",
    "    def fileList(self, extension):\n",
    "        try:\n",
    "            fileNames = os.listdir(self.directoryPath)\n",
    "            files = [os.path.splitext(file)[0] for file in fileNames if file.endswith(extension)]\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            return f\"Error occurred while reading file names\"\n",
    "        \n",
    "    def loadNextFile(self):\n",
    "        baseFileName = self.fileNames[self.fileIndex]\n",
    "        txtPath = os.path.join(self.directoryPath, baseFileName+'.txt')\n",
    "        annPath = os.path.join(self.directoryPath, baseFileName+'.ann')\n",
    "        \n",
    "        with open(txtPath, 'r') as txtFile:\n",
    "            self.text = txtFile.read()\n",
    "        \n",
    "        self.currentFileTokens = self.text.split() # simple tokenization\n",
    "        self.totalTokens = len(self.currentFileTokens)\n",
    "        \n",
    "        self.stateRepr = StateRepresentation(self.kbEmbedder, self.currentFileTokens)\n",
    "        self.entities, self.events = self.loadAnnotations(annPath)\n",
    "        self.tokenIndex = 0\n",
    "    \n",
    "    def loadAnnotations(self, annPath):\n",
    "        annFileContent = self.annotationReader.readAnnotations(annPath)\n",
    "        entities, events = self.annotationReader.parseAnnotations(annFileContent)\n",
    "        return entities, events\n",
    "        \n",
    "    def reset(self):\n",
    "        # new text\n",
    "        if self.fileIndex >= len(self.fileNames):\n",
    "            print(f\"All files were look at - going Round-Robin\")\n",
    "            self.fileIndex = 0 # reset if we have looked at all files\n",
    "        self.loadNextFile()\n",
    "        self.fileIndex += 1\n",
    "        return self.stateRepr.getState(self.tokenIndex)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.calculateReward(action, self.tokenIndex)\n",
    "        self.tokenIndex += 1\n",
    "        done = self.tokenIndex >= self.totalTokens\n",
    "        nextState = self.stateRepr.getState(self.tokenIndex) if not done else None\n",
    "        return nextState, reward, done\n",
    "    \n",
    "    def calculateReward(self, action, index):\n",
    "        self.totalIdAttempts += 1\n",
    "        isTrigger = self.isTokenTrigger(index)\n",
    "        if action == 1 and isTrigger:\n",
    "            # Positive reward for correct identification of trigger\n",
    "            self.correctIds += 1\n",
    "            return 1\n",
    "        elif action == 0 and not isTrigger:\n",
    "            # no reward for correct identification of non-trigger\n",
    "            return 0\n",
    "        else:\n",
    "            # penalize incorrect identification\n",
    "            return -1\n",
    "    \n",
    "    def isTokenTrigger(self, index):\n",
    "        # determines whether the token at the given index is a trigger\n",
    "        # In other words, get GROUND TRUTH\n",
    "        for eventId, eventInfo in self.events.items():\n",
    "            triggerId = eventInfo['trigger']\n",
    "            entity = self.entities.get(triggerId)\n",
    "            \n",
    "            if entity:\n",
    "                spanStart, spanEnd = entity['span']\n",
    "                if spanStart <= index < spanEnd:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def getTriggerIdResults(self):\n",
    "        return self.correctIds, self.totalIdAttempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f23f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLTrainer:\n",
    "    def __init__(self, agent, environment):\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "    \n",
    "    def train(self, numEpisodes):\n",
    "        reportInterval = numEpisodes/10\n",
    "        totalReward = 0\n",
    "        for episode in tqdm(range(numEpisodes)):\n",
    "            state = self.environment.reset()\n",
    "            episodeReward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.agent.chooseAction(state)\n",
    "                nextState, reward, done = self.environment.step(action)\n",
    "                self.agent.learn(state, action, reward, nextState, done)\n",
    "                state = nextState\n",
    "                episodeReward += reward\n",
    "            \n",
    "            if episode % reportInterval == 0:\n",
    "                print(f\"Episode {episode}/{numEpisodes} - Episode Reward: {episodeReward}\")\n",
    "            totalReward += episodeReward\n",
    "        correctIds, totalIdAttempts = self.environment.getTriggerIdResults()\n",
    "        print(f\"Correct Trigger Identifications: {correctIds} Total Id Attempts {totalIdAttempts}\")\n",
    "        print(f\"Training completed with totalReward {totalReward}\")\n",
    "    \n",
    "    def evaluate(self, numEpisodes):\n",
    "        totalReward = 0\n",
    "        for episode in range(numEpisodes):\n",
    "            state = self.environment.reset()\n",
    "            episodeReward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.agent.chooseAction(state)\n",
    "                nextState, reward, done = self.environment.step(action)\n",
    "                episodeReward += reward\n",
    "                state = nextState\n",
    "            totalReward += episodeReward\n",
    "        \n",
    "        averageReward = totalReward / numEpisodes\n",
    "        print(f\"Average reward: {averageReward}\")\n",
    "        return averageReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab982b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wordToVectorMap for GloVe data:\n",
      "Done - Creating wordToVectorMap for GloVe data\n",
      "Took 6.564678192138672 seconds to read GloVe data\n",
      "Processed 262 .ann files.\n",
      "Found 3536 unique tokens which correspond to entities\n",
      "Found 45 unique entities\n",
      "Initializing Parts-of-Speech Tagger\n",
      "Initialized Parts-of-Speech Tagger\n"
     ]
    }
   ],
   "source": [
    "kbEmbedder = KBAugmentedWordRepresentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f0b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationReader = AnnotationReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f70b1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlAgent = RLAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeff71f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files to read:262\n"
     ]
    }
   ],
   "source": [
    "rlEnv = RLEnvironment(kbEmbedder, annotationReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a8b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RLTrainer(rlAgent, rlEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a26f192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                      | 1/262 [00:24<1:46:22, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/262 - Episode Reward: -128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 262/262 [2:50:01<00:00, 38.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Trigger Identifications: 1998 Total Id Attempts 56588\n",
      "Training completed with totalReward -24928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainingEpisodes = 262\n",
    "trainer.train(trainingEpisodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
